{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. FCN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqkKav8O6Fh1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os.path as osp\n",
    "import PIL\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device('cuda')\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset via wget\n",
    "\n",
    "wget은 web-get의 약자로, 웹에서 파일을 직접 가져오는 명령어입니다. 우리는 데이터가 있는 웹 주소에서 데이터를 직접 가져와 tar.gz 형태의 파일로 다운로드 받을 수 있도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Downloading dataset from google drive, 밑의 코드의 주석을 풀면 구글 드라이브로 부터 데이터셋 다운, 압축이 풀리고, Kitti라는 폴더가 생성됩니다. \n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z\" -O Kitti.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip Kitti.zip -d ./data/Kitti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일을 제대로 다운로드 하였고 읽을 수 있는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "error",
     "timestamp": 1636096005436,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "TwA5VAjp6Fh7",
    "outputId": "24ad80d0-4595-45b5-b8c4-c7b3da462966",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgsets_file = osp.join('./data/Kitti', '{}.txt'.format('train'))\n",
    "for line in open(imgsets_file):\n",
    "    line = line.strip()\n",
    "    print(line)\n",
    "    line = line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEEnAaqx6Fh8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KITTIdataset(torch.utils.data.Dataset):\n",
    "    class_names = np.array(['background', 'road'])\n",
    "\n",
    "    def __init__(self, root, transform, split='train'):\n",
    "        self.root = ## implement code here\n",
    "        self.split = ## implement code here\n",
    "        self.transform = ## implement code here\n",
    "\n",
    "        self.image_path = []\n",
    "        self.ys = []\n",
    "        \n",
    "        imgsets_file = osp.join(root, '{}.txt'.format(split))\n",
    "        for did in open(imgsets_file):\n",
    "            did = did.strip()\n",
    "            did = did.split()\n",
    "            img_file = osp.join(root, 'data_road/{}'.format(did[0]))\n",
    "            lbl_file = osp.join(root, 'data_road/{}'.format(did[1]))\n",
    "            self.image_path.append(img_file)\n",
    "            self.ys.append(lbl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        img_file = ## implement code here\n",
    "        img = PIL.Image.open(img_file)\n",
    "        \n",
    "        # load label\n",
    "        lbl_file = ## implement code here\n",
    "        lbl = PIL.Image.open(lbl_file)\n",
    "        lbl = np.array(lbl)\n",
    "        lbl[lbl == 255] = 1 # 0 is black 255 is white\n",
    "        \n",
    "        return self.transform(img), torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "train_dataset = KITTIdataset(root = './data/Kitti', split = 'train', transform = transform)\n",
    "val_dataset = KITTIdataset(root = './data/Kitti', split = 'val', transform = transform)\n",
    "\n",
    "train_loader = ## implement code here (use torch.utils.data.DataLoader, batch_size=1)\n",
    "val_loader = ## implement code here (use torch.utils.data.DataLoader, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation matric (mIoU)\n",
    "\n",
    "The evaluation matric code is given.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_mean_iou(label_trues, label_preds, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iou = np.nanmean(iu)\n",
    "    \n",
    "    return mean_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtKS8ll6Fh-"
   },
   "source": [
    "# Define the Network\n",
    "\n",
    "- FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, num_class=3):\n",
    "        super().__init__()\n",
    "        self.loss = ## Apply cross entropy loss\n",
    "        self.num_class = ## implement code here\n",
    "    \n",
    "        #############################################################\n",
    "        # Structure of the FCN model\n",
    "        #\n",
    "        #3->64 2\n",
    "        #64->128 2\n",
    "        #128->256 3 conv->relu->conv->relu->conv->relu => Predict 3\n",
    "        #256->512 3 conv->relu->conv->relu->conv->relu => Predict 2\n",
    "        #512->512 3 conv->relu->conv->relu->conv->relu\n",
    "        #512->4096 2 conv->relu->conv->relu => Predict 1\n",
    "        #############################################################\n",
    "        \n",
    "        ## conv1\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding = 100), # the padding=100 is given for a reason! Other conv2d should all have padding=1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding =1),\n",
    "            nn.ReLU())\n",
    "        ## pool1\n",
    "        \n",
    "        ## conv2\n",
    "        self.features2 = nn.Sequential() ## implement code here\n",
    "        ## pool2\n",
    "        \n",
    "        ## conv3\n",
    "        self.features3 = nn.Sequential() ## implement code here\n",
    "        ## pool3\n",
    "\n",
    "        ## conv4\n",
    "        self.features4 = nn.Sequential() ## implement code here\n",
    "        ## pool4\n",
    "        \n",
    "        ## conv5\n",
    "        self.features5 = nn.Sequential() ## implement code here\n",
    "        \n",
    "        self.maxpool = ## implement code here (stride=2, ceil_mode=True)\n",
    "        \n",
    "        #4096->4096->num_class\n",
    "        self.classifier = nn.Sequential() # conv(kernel_size=7) - relu - dropout - conv(kernel_size=1) - relu - dropout - conv(kernel_size=1)\n",
    "        \n",
    "        ## pool 5\n",
    "\n",
    "\n",
    "        ## upsampling transposed convolution (use nn.ConvTranspose2d, in&out channel:num_class)\n",
    "        self.upscore2 =  # kernel:4, strid:2\n",
    "        self.upscore4 =  # kernel:4, strid:2\n",
    "        self.upscore8 =  # kernel:16, strid:8\n",
    "        \n",
    "        self.score_pool4 = ## conv for Predict 2\n",
    "        self.score_pool3 = ## conv for Predict 3\n",
    "\n",
    "        self.softmax = ## implement code here\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #################################\n",
    "        ## implement code here\n",
    "        ## 230628_Segmentation.pdf Exercise 1에 나온 모델 구조 그림을 참고하세요.\n",
    "\n",
    "        x1 = \n",
    "        pool1 = \n",
    "\n",
    "        x2 = \n",
    "        pool2 = \n",
    "\n",
    "        x3 = \n",
    "        pool3 = \n",
    "\n",
    "        x4 = \n",
    "        pool4 = \n",
    "\n",
    "        x5 = \n",
    "        pool5 = \n",
    "\n",
    "        predict1 = \n",
    "        \n",
    "\n",
    "        deconv1 = \n",
    "        predict2 = \n",
    "        predict2 = predict2[:, :, 5:5 + deconv1.size()[2], 5:5 + deconv1.size()[3]] # 사이즈 조절을 위함\n",
    "        add1 = # use torch.add() to add two feature maps\n",
    "\n",
    "        deconv2 = \n",
    "        predict3 = \n",
    "        predict3 = predict3[:, :, 9:9 + deconv2.size()[2], 9:9 + deconv2.size()[3]] # 사이즈 조절을 위함\n",
    "        add2 = # use torch.add() to add two feature maps\n",
    "\n",
    "\n",
    "        deconv3 = \n",
    "        deconv3 = deconv3[:, :, 33:33 + x.size()[2], 33:33 + x.size()[3]] # 사이즈 조절을 위함\n",
    "        out = \n",
    "        ##################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (num_class:2)\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = ## use cross entropy loss\n",
    "optimizer = ## use Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 5\n",
    "best_iou = 0\n",
    "num_class = len(train_loader.dataset.class_names)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    print ('current epoch : %d'%(epoch))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load data, forward\n",
    "        # data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score = model(data)\n",
    "\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 20 ==0:\n",
    "            print ('batch : {}, loss : {}'.format(batch_idx, loss.item()))\n",
    "\n",
    "        \n",
    "    #validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # load data, forward\n",
    "            # data, target = data.cuda(), target.cuda()\n",
    "            score = model(data)\n",
    "\n",
    "            # calc val loss, accuracy\n",
    "            loss = criterion(score, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, lbl_pred = score.max(1)\n",
    "            lbl_pred = lbl_pred.cpu().numpy()  \n",
    "            lbl_true = target.cpu().numpy()\n",
    "\n",
    "            for lt, lp in zip(lbl_true, lbl_pred): # lbl_true, lbl_pred: [batch, h, w]\n",
    "                tmp = compute_mean_iou(lt, lp, num_class)\n",
    "                metrics.append(tmp)\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    metrics = np.mean(metrics)\n",
    "    \n",
    "    print ('val loss : {}, mean_iou : {}'.format(val_loss, metrics))\n",
    "\n",
    "    ##save model\n",
    "    if best_iou < metrics:\n",
    "        best_iou = metrics\n",
    "        print(\"Best model saved\")\n",
    "        torch.save(model.state_dict(), './model_best.pth')\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. U-Net implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(pl.LightningModule):\n",
    "    def __init__(self, num_class=3):\n",
    "        super().__init__()\n",
    "        self.loss = ## apply cross entropy loss\n",
    "        self.num_class = ## implement code here\n",
    "\n",
    "        ##################################################\n",
    "        # 자유롭게 U-Net 구조를 만들어보세요.\n",
    "        # 230628_Segmentation.pdf 파일의 Exercise 2. U-Net implementation 설명 figure를 참고하셔도 좋습니다.\n",
    "        # 간소한 버전으로 구현을 하고, 최대한 skip-connection을 사용하는 방법을 생각해보세요.\n",
    "        \n",
    "        # self.@@@ = nn.@@@@\n",
    "        # ...\n",
    "\n",
    "\n",
    "        ##################################################\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ##################################################\n",
    "        # 자유롭게 U-Net 구조를 만들어보세요.\n",
    "        # 230628_Segmentation.pdf 파일의 Exercise 2. U-Net implementation 설명 figure를 참고하셔도 좋습니다.\n",
    "        # 간소한 버전으로 구현을 하고, 최대한 skip-connection을 사용하는 방법을 생각해보세요.\n",
    "        \n",
    "        # x1 = self.@@@(x)\n",
    "        # ...\n",
    "\n",
    "\n",
    "        ##################################################\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet 코드는 실행되지 않아도 좋습니다. U-Net 구조와 Skip-connection의 활용법을 배우는 것이 목표입니다.  \n",
    "(UNet 코드의 경우 데이터의 사이즈 문제 등으로 그대로 실행시 돌아가지 않을 가능성이 큽니다.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "seg_kitti_ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cee20aa2885cadc07e824ce5082d40bca942426616eda434cad5578791d33ff8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
